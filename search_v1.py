import subprocess
import sys
import requests
import json
import time
import random
import urllib
from bs4 import BeautifulSoup
import csv


def install_packages():
    """í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."""
    try:
        print("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (beautifulsoup4, requests) ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "beautifulsoup4", "requests"])
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except subprocess.CalledProcessError as e:
        print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ 'pip install beautifulsoup4 requests' ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        sys.exit(1)


# --- 1. ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ---
TOP_LEVEL_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000",
    "íŒ¨ì…˜ì¡í™”": "50000001",
    "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003",
    "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004",
    "ì¶œì‚°/ìœ¡ì•„": "50000005",
    "ì‹í’ˆ": "50000006",
    "ìŠ¤í¬ì¸ /ë ˆì €": "50000007",
    "ìƒí™œ/ê±´ê°•": "50000008",
    "ì—¬ê°€/ìƒí™œí¸ì˜": "50000009",
    "ë©´ì„¸ì ": "50000010",
    "ë„ì„œ": "50005542"
}

def search_naver_rank(food_cid):
    print(f"ì‹í’ˆ ì¹´í…Œê³ ë¦¬ ID: {food_cid}")
    # ì¶œë ¥: ì‹í’ˆ ì¹´í…Œê³ ë¦¬ ID: 50000006
    # 1. ìš”ì²­ì„ ë³´ë‚¼ URL
    url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"

    # 2. Headers ì •ë³´ ì„¤ì • (User-AgentëŠ” ë³¸ì¸ ê²ƒìœ¼ë¡œ êµì²´í•˜ëŠ” ê²ƒì„ ê¶Œì¥)
    #    ê°œë°œì ë„êµ¬ -> Network -> getCategoryKeywordRank.naver -> Headers -> Request Headers ì—ì„œ ë³µì‚¬
    headers = {
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://datalab.naver.com/shoppingInsight/sCategory.naver",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }
    dic1 = {}

    # 3. Payload (Form Data) ì •ë³´ ì„¤ì • - ì›í•˜ëŠ” ì¡°ê±´ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©
    #    'íŒ¨ì…˜ì˜ë¥˜(50000000)' ì¹´í…Œê³ ë¦¬ì˜ 2024ë…„ 1ì›” í•œ ë‹¬ê°„ ì „ì²´ ì¸ê¸° ê²€ìƒ‰ì–´ ìˆœìœ„
    for a in range(1, 3):
        payload = {
            "cid": food_cid,
            "timeUnit": "date",  # ì›”ê°„ ë‹¨ìœ„
            "startDate": "2025-08-28",
            "endDate": "2025-08-29",
            "age": "",  # ì „ì²´ ì—°ë ¹
            "gender": "",  # ì „ì²´ ì„±ë³„
            "device": "",  # ì „ì²´ ê¸°ê¸°
            "page": a,
        }

        # 4. POST ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(url, headers=headers, data=payload)

        # 5. ì‘ë‹µ í™•ì¸ ë° ë°ì´í„° íŒŒì‹±
        if response.status_code == 200:
            try:
                # ì‘ë‹µ ë°›ì€ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ íŒŒì‹±
                data = response.json()

                # ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ (indent=2)
                print(json.dumps(data, indent=2, ensure_ascii=False))

                # ìˆœìœ„ì™€ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ì¶œë ¥
                # print("\n--- ì¸ê¸° ê²€ìƒ‰ì–´ ìˆœìœ„ ---")
                for item in data.get('ranks', []):
                    dic1[item.get('rank')] = item.get('keyword')
                    # print(f"{item['rank']}. {item['keyword']}")

            except json.JSONDecodeError:
                print("JSON ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                print("ì‘ë‹µ ë‚´ìš©:", response.text)
        else:
            print(f"ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
    return dic1


# --- 2. 'ì‹¸ë‹¤êµ¬' ìƒí’ˆ ê²€ìƒ‰ ê´€ë ¨ í•¨ìˆ˜ ---
def search_products_ssadagu(search_term, page=1):
    """ì‹¸ë‹¤êµ¬ (ssadagu.kr) ì‚¬ì´íŠ¸ì—ì„œ ìƒí’ˆì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    search_url = "https://ssadagu.kr/shop/ajax.infinity_shop_list.php"
    payload = {
        'page_div_id': 'infinity_item_list', 'page_type': 'pc', 'ss_tx': search_term,
        'search_option_array': ['activeType'], 'hi_platform': '1688', 'sort_item': 'default', 'page': page
    }
    encoded_q = urllib.parse.quote(search_term, safe="")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'X-Requested-With': 'XMLHttpRequest',
        'Referer': f'https://ssadagu.kr/shop/search.php?ss_tx={encoded_q}'
    }
    try:
        response = requests.post(search_url, data=payload, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"HTTP ìš”ì²­ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None
    except json.JSONDecodeError:
        print("JSON ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.");
        print("ì‘ë‹µ ë‚´ìš©:", response.text)
        return None






# --- 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    install_packages()

    print("\nâœ… ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì—ì„œ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")

    # ìµœìƒìœ„ ì¹´í…Œê³ ë¦¬ ì´ë¦„ë“¤ ì¤‘ì—ì„œ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
    top_level_names = list(TOP_LEVEL_CATEGORIES.keys())
    search_categories = random.choice(top_level_names)
    print(f"ğŸŒŸ ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: '{search_categories}'")
    dic = search_naver_rank(TOP_LEVEL_CATEGORIES[search_categories])
    search_keyword = random.choice(dic)

    # 'ì‹¸ë‹¤êµ¬'ì—ì„œ ìƒí’ˆ ê²€ìƒ‰ ë° CSV ì €ì¥
    result_data = search_products_ssadagu(search_keyword)

    if result_data and result_data.get('success'):
        product_html_list = result_data.get('data', [])
        print(f"ğŸ“„ ì´ {len(product_html_list)}ê°œì˜ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")

        if not product_html_list:
            print(f"'{search_keyword}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ CSV íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            safe_keyword = "".join(c for c in search_keyword if c.isalnum() or c in (' ', '_')).rstrip()
            csv_filename = f'{safe_keyword}_ssadagu_products.csv'

            with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                csv_writer = csv.writer(csvfile)
                header = ['ìƒí’ˆID', 'ìƒí’ˆëª…', 'ê°€ê²©(ì›)', 'ë§í¬', 'ì´ë¯¸ì§€URL']
                csv_writer.writerow(header)

                for product_html in product_html_list:
                    soup = BeautifulSoup(product_html, 'html.parser')
                    li_tag = soup.find('li')
                    if not li_tag: continue
                    gs_id = li_tag.get('data-gs-id', 'N/A')
                    title = li_tag.get('data-title', 'N/A')
                    image_url = li_tag.get('data-img-url', 'N/A')
                    price_tag = soup.select_one('.product_price')
                    price_krw = price_tag.text.replace('ì›', '').strip() if price_tag else 'N/A'
                    link_tag = soup.select_one('.product_image a')
                    product_url = link_tag['href'] if link_tag and link_tag.has_attr('href') else 'N/A'
                    csv_writer.writerow([gs_id, title, price_krw, product_url, image_url])

            print(f"ğŸ‰ ìƒí’ˆ ì •ë³´ê°€ '{csv_filename}' íŒŒì¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âŒ '{search_keyword}' ìƒí’ˆ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")