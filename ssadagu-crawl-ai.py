import requests
import urllib.parse
from bs4 import BeautifulSoup
import re
import json
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import subprocess
import sys
import csv

from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ì´ë¯¸ì§€ ë¶„ì„ìš© ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image
import io

# JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ì¸ì½”ë” í´ë˜ìŠ¤ ì¶”ê°€
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

# SSADAGUCrawler í´ë˜ìŠ¤ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ)
class SSADAGUCrawler:
    def __init__(self, use_selenium=True):
        self.base_url = "https://ssadagu.kr"
        self.use_selenium = use_selenium
        if use_selenium:
            self.setup_selenium()
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
    def setup_selenium(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, 10)
            print("Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("requests ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.use_selenium = False
            self.session = requests.Session()

    def search_products_selenium(self, keyword):
        """Seleniumì„ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰ - ê¸°ì¡´ ë°©ì‹ ìœ ì§€"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            self.driver.get(search_url)
            time.sleep(5)
            product_links = []
            link_elements = self.driver.find_elements(By.TAG_NAME, "a")
            for element in link_elements:
                href = element.get_attribute('href')
                if href and 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    product_links.append(href)
            product_links = list(set(product_links))
            print(f"Seleniumìœ¼ë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"Selenium ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_products_requests(self, keyword):
        """requestsë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰ - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³µì›"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            response = self.session.get(search_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            product_links = []
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                if 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                    product_links.append(full_url)
            print(f"requestsë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"requests ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def crawl_product_basic(self, product_url):
        """ê¸°ë³¸ ìƒí’ˆ ì •ë³´ë§Œ í¬ë¡¤ë§ (ìœ ì‚¬ë„ ë¶„ì„ìš©)"""
        try:
            if self.use_selenium:
                self.driver.get(product_url)
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            title_element = soup.find('h1', {'id': 'kakaotitle'})
            title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
            
            price = 0
            price_selectors = [
                'span.price.gsItemPriceKWR',
                '.pdt_price span.price',
                'span.price',
                '.price'
            ]
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        price = int(price_match.group(1))
                        break

            rating = self.calculate_rating(soup)
            
            return {
                'url': product_url,
                'title': title,
                'price': price,
                'rating': rating
            }
        except Exception as e:
            print(f"ê¸°ë³¸ ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def crawl_product_detail(self, product_url, include_images=True):
        """ìƒì„¸ ìƒí’ˆ ì •ë³´ í¬ë¡¤ë§ (ìµœì¢… ì„ íƒëœ ìƒí’ˆìš©)"""
        try:
            if self.use_selenium:
                self.driver.get(product_url)
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            title_element = soup.find('h1', {'id': 'kakaotitle'})
            title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
            
            price = 0
            price_selectors = [
                'span.price.gsItemPriceKWR',
                '.pdt_price span.price',
                'span.price',
                '.price'
            ]
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        price = int(price_match.group(1))
                        break

            rating = self.calculate_rating(soup)
            options = self.extract_product_options(soup)
            material_info = self.extract_material_info(soup)
            
            product_data = {
                'url': product_url,
                'title': title,
                'price': price,
                'rating': rating,
                'options': options,
                'material_info': material_info,
                'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” ì„ íƒì ìœ¼ë¡œë§Œ ì§„í–‰
            if include_images:
                print("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                product_images = self.extract_product_images(soup)
                translated_images = []
                for img_url in product_images:
                    translated_text = ocr_and_translate_image(img_url)
                    translated_images.append({
                        'original_url': img_url,
                        'translated_text': translated_text
                    })
                product_data['product_images'] = translated_images
            else:
                product_data['product_images'] = []
                
            return product_data
        except Exception as e:
            print(f"ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def calculate_rating(self, soup):
        rating = 0.0
        star_containers = [
            soup.find('a', class_='start'),
            soup.find('div', class_=re.compile(r'star|rating')),
            soup.find('a', href='#reviews_wrap')
        ]
        for container in star_containers:
            if container:
                star_imgs = container.find_all('img')
                for img in star_imgs:
                    src = img.get('src', '')
                    if 'icon_star.svg' in src:
                        rating += 1
                    elif 'icon_star_half.svg' in src:
                        rating += 0.5
                break
        return rating

    def extract_product_options(self, soup):
        options = []
        sku_list = soup.find('ul', {'id': 'skubox'})
        if sku_list:
            option_items = sku_list.find_all('li', class_=re.compile(r'imgWrapper'))
            for item in option_items:
                title_element = item.find('a', title=True)
                if title_element:
                    option_name = title_element.get('title', '').strip()
                    stock = 0
                    item_text = item.get_text()
                    stock_match = re.search(r'ì¬ê³ \s*:\s*(\d+)', item_text)
                    if stock_match:
                        stock = int(stock_match.group(1))
                    img_element = item.find('img', class_='colorSpec_hashPic')
                    image_url = ""
                    if img_element and img_element.get('src'):
                        image_url = img_element['src']
                    if option_name:
                        options.append({
                            'name': option_name,
                            'stock': stock,
                            'image_url': image_url
                        })
        return options

    def extract_product_images(self, soup):
        images = []
        img_elements = soup.find_all('img', {'id': re.compile(r'img_translate_\d+')})
        for img in img_elements:
            src = img.get('src', '')
            if src:
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = self.base_url + src
                elif src.startswith('http'):
                    pass
                else:
                    continue
                images.append(src)
        return images

    def extract_material_info(self, soup):
        material_info = {}
        info_items = soup.find_all('div', class_='pro-info-item')
        for item in info_items:
            title_element = item.find('div', class_='pro-info-title')
            info_element = item.find('div', class_='pro-info-info')
            if title_element and info_element:
                title = title_element.get_text(strip=True)
                info = info_element.get_text(strip=True)
                material_info[title] = info
        return material_info

    def crawl_search_results(self, keyword, max_products=20):
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìƒí’ˆë“¤ í¬ë¡¤ë§ - ê¸°ì¡´ ë°©ì‹ ìœ ì§€"""
        print(f"'{keyword}' ê²€ìƒ‰ ì‹œì‘...")
        if self.use_selenium:
            product_links = self.search_products_selenium(keyword)
        else:
            product_links = self.search_products_requests(keyword)
        
        if not product_links:
            print("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"{len(product_links)}ê°œì˜ ìƒí’ˆ ë§í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        crawled_products = []
        for i, link in enumerate(product_links[:max_products]):
            print(f"\nìƒí’ˆ {i+1}/{min(len(product_links), max_products)} í¬ë¡¤ë§ ì¤‘...")
            product_data = self.crawl_product_detail(link, include_images=True)
            if product_data:
                crawled_products.append(product_data)
                print(f"âœ“ í¬ë¡¤ë§ ì„±ê³µ: {product_data['title'][:50]}...")
            else:
                print("âœ— í¬ë¡¤ë§ ì‹¤íŒ¨")
            time.sleep(random.uniform(2, 4))
        return crawled_products

    def __del__(self):
        if hasattr(self, 'driver'):
            try:
                self.driver.quit()
            except:
                pass

# AI ëª¨ë¸ì„ í™œìš©í•œ ìœ ì‚¬ë„ ë¶„ì„ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ)
class SimilarityAnalyzer:
    def __init__(self):
        try:
            # ë” ì•ˆì •ì ì¸ í•œêµ­ì–´ BERT ëª¨ë¸ ì‚¬ìš©
            self.tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
            self.model = AutoModel.from_pretrained('klue/bert-base')
            print("KLUE BERT ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        except Exception as e:
            print(f"KLUE BERT ë¡œë”© ì‹¤íŒ¨, ë‹¤êµ­ì–´ BERTë¡œ ëŒ€ì²´: {e}")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
                self.model = AutoModel.from_pretrained('bert-base-multilingual-cased')
                print("ë‹¤êµ­ì–´ BERT ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e2:
                print(f"ëª¨ë“  BERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e2}")
                raise e2
    
    def get_embedding(self, text):
        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :].numpy()

    def get_similarity(self, text1, text2):
        embedding1 = self.get_embedding(text1)
        embedding2 = self.get_embedding(text2)
        return cosine_similarity(embedding1, embedding2)[0][0]

# 2ë‹¨ê³„ ê²€ì¦ ë¶„ì„ê¸° (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
class DualVerificationAnalyzer(SimilarityAnalyzer):
    """í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ 2ë‹¨ê³„ ê²€ì¦ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # ê¸°ì¡´ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì´ˆê¸°í™”
        super().__init__()
        
        # ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ì¶”ê°€
        self.setup_image_analyzer()
        
    def setup_image_analyzer(self):
        """ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print("ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.image_model = models.resnet50(pretrained=True)
            self.image_model.eval()
            
            # ë§ˆì§€ë§‰ ë¶„ë¥˜ì¸µ ì œê±° (íŠ¹ì„± ë²¡í„°ë§Œ ì¶”ì¶œ)
            self.image_model = torch.nn.Sequential(*list(self.image_model.children())[:-1])
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            print("âœ“ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            print(f"âœ— ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.image_model = None
    
    def search_naver_images(self, keyword, num_images=3):
        """ë„¤ì´ë²„ì—ì„œ í‚¤ì›Œë“œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        print(f"  ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹œë„: {keyword}")
        encoded_keyword = urllib.parse.quote(keyword)
        naver_url = f"https://search.naver.com/search.naver?where=image&query={encoded_keyword}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        try:
            response = requests.get(naver_url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_urls = []
            
            # ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì…€ë ‰í„° ì‹œë„
            selectors = [
                'img._img',
                'img[class*="img"]',
                'img[src*="pstatic"]',
                'img[data-src*="pstatic"]'
            ]
            
            for selector in selectors:
                img_elements = soup.select(selector)
                print(f"  ì…€ë ‰í„° '{selector}'ë¡œ {len(img_elements)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                
                for img in img_elements[:num_images]:
                    img_src = img.get('src') or img.get('data-src')
                    if img_src and img_src.startswith('http'):
                        image_urls.append(img_src)
                        print(f"    ì´ë¯¸ì§€ URL ì¶”ê°€: {img_src[:50]}...")
                        
                if image_urls:
                    break
                    
            # ì¤‘ë³µ ì œê±°
            image_urls = list(set(image_urls))[:num_images]
            print(f"  ìµœì¢… ìˆ˜ì§‘ëœ ì´ë¯¸ì§€: {len(image_urls)}ê°œ")
            return image_urls
            
        except Exception as e:
            print(f"  ë„¤ì´ë²„ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def download_and_process_image(self, image_url):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': 'https://www.google.com/'
            }
            
            print(f"    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„: {image_url[:50]}...")
            response = requests.get(image_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            print(f"    ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
            image = Image.open(io.BytesIO(response.content)).convert('RGB')
            print(f"    ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
            
            image_tensor = self.transform(image).unsqueeze(0)
            print(f"    í…ì„œ í˜•íƒœ: {image_tensor.shape}")
            
            return image_tensor
            
        except Exception as e:
            print(f"    ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_image_features(self, image_tensor):
        """ì´ë¯¸ì§€ì—ì„œ íŠ¹ì„± ë²¡í„° ì¶”ì¶œ"""
        if self.image_model is None:
            return None
            
        try:
            with torch.no_grad():
                features = self.image_model(image_tensor)
                features = features.view(features.size(0), -1)
                return features.numpy()
        except Exception as e:
            return None
    
    def get_reference_features(self, keyword):
        """ë„¤ì´ë²„ ê²€ìƒ‰ ì´ë¯¸ì§€ë“¤ì˜ í‰ê·  íŠ¹ì„± ë²¡í„° ê³„ì‚°"""
        if self.image_model is None:
            print("  ì´ë¯¸ì§€ ëª¨ë¸ì´ ì—†ìŒ")
            return None
            
        naver_images = self.search_naver_images(keyword, num_images=3)
        
        if not naver_images:
            print("  ë„¤ì´ë²„ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨")
            return None
            
        reference_features = []
        
        for i, img_url in enumerate(naver_images):
            print(f"  ì°¸ì¡° ì´ë¯¸ì§€ {i+1}/{len(naver_images)} ì²˜ë¦¬ ì¤‘...")
            image_tensor = self.download_and_process_image(img_url)
            if image_tensor is not None:
                features = self.extract_image_features(image_tensor)
                if features is not None:
                    reference_features.append(features)
                    print(f"    íŠ¹ì„± ì¶”ì¶œ ì„±ê³µ: {features.shape}")
                else:
                    print(f"    íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨")
            else:
                print(f"    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                    
        if reference_features:
            avg_features = np.mean(reference_features, axis=0)
            print(f"  ìµœì¢… í‰ê·  íŠ¹ì„±: {avg_features.shape}, ì„±ê³µí•œ ì´ë¯¸ì§€: {len(reference_features)}ê°œ")
            return avg_features
        else:
            print("  ëª¨ë“  ì°¸ì¡° ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
            return None

# 2ë‹¨ê³„ ê²€ì¦ í¬ë¡¤ëŸ¬ (ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜)
class DualVerificationCrawler(SSADAGUCrawler):
    """2ë‹¨ê³„ ê²€ì¦ í¬ë¡¤ëŸ¬ (ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜)"""
    
    def __init__(self, use_selenium=True):
        super().__init__(use_selenium)
        
    def extract_product_image_urls(self, soup):
        """ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        image_urls = []
        
        # ê¸°ì¡´ extract_product_images ë©”ì„œë“œ í™œìš©
        img_elements = soup.find_all('img', {'id': re.compile(r'img_translate_\d+')})
        for img in img_elements:
            src = img.get('src', '')
            if src:
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = self.base_url + src
                elif src.startswith('http'):
                    pass
                else:
                    continue
                image_urls.append(src)
                
        return image_urls[:2]  # ì²˜ìŒ 2ê°œë§Œ
    
    def calculate_image_similarity_for_product(self, analyzer, reference_features, product_url):
        """íŠ¹ì • ìƒí’ˆì˜ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
        if reference_features is None:
            print("    ì°¸ì¡° íŠ¹ì„± ì—†ìŒ")
            return 0.0
            
        if analyzer.image_model is None:
            print("    ì´ë¯¸ì§€ ëª¨ë¸ ì—†ìŒ")
            return 0.0
            
        try:
            # ìƒí’ˆ í˜ì´ì§€ ë¡œë”©
            if self.use_selenium:
                self.driver.get(product_url)
                time.sleep(2)
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìƒí’ˆ ì´ë¯¸ì§€ URL ì¶”ì¶œ
            product_image_urls = self.extract_product_image_urls(soup)
            print(f"    ìƒí’ˆ ì´ë¯¸ì§€ {len(product_image_urls)}ê°œ ë°œê²¬")
            
            if not product_image_urls:
                print("    ìƒí’ˆ ì´ë¯¸ì§€ ì—†ìŒ")
                return 0.0
                
            product_features = []
            
            for i, img_url in enumerate(product_image_urls):
                print(f"    ìƒí’ˆ ì´ë¯¸ì§€ {i+1}/{len(product_image_urls)} ì²˜ë¦¬ ì¤‘...")
                image_tensor = analyzer.download_and_process_image(img_url)
                if image_tensor is not None:
                    features = analyzer.extract_image_features(image_tensor)
                    if features is not None:
                        product_features.append(features)
                        print(f"      íŠ¹ì„± ì¶”ì¶œ ì„±ê³µ")
                    else:
                        print(f"      íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨")
                else:
                    print(f"      ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                        
            if not product_features:
                print("    ëª¨ë“  ìƒí’ˆ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
                return 0.0
                
            # ìƒí’ˆ ì´ë¯¸ì§€ë“¤ì˜ í‰ê·  íŠ¹ì„±
            avg_product_features = np.mean(product_features, axis=0)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity(
                reference_features.reshape(1, -1), 
                avg_product_features.reshape(1, -1)
            )[0][0]
            
            print(f"    ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {similarity:.4f}")
            return max(0.0, similarity)
            
        except Exception as e:
            print(f"    ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

# ì´ë¯¸ì§€ ë²ˆì—­ ê¸°ëŠ¥ (ê¸°ì¡´ í•¨ìˆ˜)
def ocr_and_translate_image(image_url):
    print(f"ì´ë¯¸ì§€ ë²ˆì—­ ì‹œë„: {image_url}")
    try:
        translated_text = "ë²ˆì—­ëœ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì˜ˆì‹œì…ë‹ˆë‹¤."
        print(f"âœ“ ë²ˆì—­ ì„±ê³µ: '{translated_text[:20]}...'")
        return translated_text
    except Exception as e:
        print(f"âœ— ì´ë¯¸ì§€ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return None

# ì„¤ì¹˜ í•¨ìˆ˜ ë° ë„¤ì´ë²„ ë°ì´í„°ë© í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ)
def install_packages():
    try:
        print("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        packages = [
            "beautifulsoup4", 
            "requests", 
            "selenium", 
            "torch", 
            "transformers", 
            "numpy", 
            "scikit-learn",
            "protobuf",
            "torchvision",
            "pillow"
        ]
        subprocess.check_call([sys.executable, "-m", "pip", "install"] + packages)
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except subprocess.CalledProcessError as e:
        print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("pip install beautifulsoup4 requests selenium torch transformers numpy scikit-learn protobuf torchvision pillow")
        sys.exit(1)

TOP_LEVEL_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000", "íŒ¨ì…˜ì¡í™”": "50000001", "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003", "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004", "ì¶œì‚°/ìœ¡ì•„": "50000005",
    "ì‹í’ˆ": "50000006", "ìŠ¤í¬ì¸ /ë ˆì €": "50000007", "ìƒí™œ/ê±´ê°•": "50000008",
    "ì—¬ê°€/ìƒí™œí¸ì˜": "50000009", "ë©´ì„¸ì ": "50000010", "ë„ì„œ": "50005542"
}

def search_naver_rank(category_id):
    url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    headers = {
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://datalab.naver.com/shoppingInsight/sCategory.naver",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }
    keywords = []
    payload = {
        "cid": category_id,
        "timeUnit": "date",
        "startDate": "2025-08-28",
        "endDate": "2025-08-29",
        "age": "",
        "gender": "",
        "device": "",
        "page": 1,
    }
    try:
        response = requests.post(url, headers=headers, data=payload)
        response.raise_for_status()
        data = response.json()
        for item in data.get('ranks', []):
            keywords.append(item.get('keyword'))
    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
        print(f"ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    return keywords

# ê¸°ì¡´ main_merged í•¨ìˆ˜ (fallbackìš©)
def main_merged():
    install_packages()
    print("\n=== SSADAGU í†µí•© í¬ë¡¤ëŸ¬ ===")

    crawler = SSADAGUCrawler(use_selenium=True)
    analyzer = SimilarityAnalyzer()

    SIMILARITY_THRESHOLD = 0.7
    MAX_RETRY = 5

    best_match_product = None
    best_match_url = None
    max_similarity = 0.0

    for attempt in range(MAX_RETRY):
        category_name = random.choice(list(TOP_LEVEL_CATEGORIES.keys()))
        category_id = TOP_LEVEL_CATEGORIES[category_name]
        trending_keywords = search_naver_rank(category_id)

        keyword = random.choice(trending_keywords) if trending_keywords else "ì•…ì„¸ì‚¬ë¦¬"
        print(f"\n[{attempt+1}/{MAX_RETRY}] ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {category_name}, í‚¤ì›Œë“œ: {keyword}")

        search_results_urls = (
            crawler.search_products_selenium(keyword)
            if crawler.use_selenium
            else crawler.search_products_requests(keyword)
        )

        if not search_results_urls:
            print("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„")
            continue

        try:
            keyword_embedding = analyzer.get_embedding(keyword)
            temp_best_url = None
            temp_max_sim = 0.0

            for i, url in enumerate(search_results_urls[:20]):
                basic_data = crawler.crawl_product_basic(url)
                if not basic_data:
                    continue
                if basic_data['title'] == "ì œëª© ì—†ìŒ":
                    print("ì œëª© ì—†ìŒ â†’ ìŠ¤í‚µ")
                    continue

                title_embedding = analyzer.get_embedding(basic_data['title'])
                similarity = cosine_similarity(keyword_embedding, title_embedding)[0][0]
                print(f"ìƒí’ˆ {i+1}: {basic_data['title'][:30]} | ìœ ì‚¬ë„: {similarity:.4f}")

                if similarity > temp_max_sim:
                    temp_max_sim = similarity
                    temp_best_url = url

            if temp_max_sim >= SIMILARITY_THRESHOLD:
                best_match_url = temp_best_url
                max_similarity = temp_max_sim
                break
            else:
                print(f"ìœ ì‚¬ë„ {temp_max_sim:.4f} â†’ ê¸°ì¤€ ë¯¸ë‹¬, ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„")

        except Exception as e:
            print(f"ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            continue

    if best_match_url:
        best_match_product = crawler.crawl_product_detail(best_match_url, include_images=True)
        if best_match_product:
            print(f"\nìµœì¢… ìƒí’ˆ: {best_match_product['title']}")
            print(f"ê°€ê²©: {best_match_product['price']}ì› | ìœ ì‚¬ë„: {max_similarity:.4f}")

            output_filename = f"ssadagu_best_match_{int(time.time())}.json"
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump([best_match_product], f, ensure_ascii=False, indent=2)
            print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_filename}")
        else:
            print("ìµœì¢… ìƒí’ˆ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")
    else:
        print("ì¶”ì²œí•  ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return best_match_product

# 2ë‹¨ê³„ ê²€ì¦ ë©”ì¸ í•¨ìˆ˜ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
def main_dual_verification():
    """2ë‹¨ê³„ ê²€ì¦ ë©”ì¸ í•¨ìˆ˜ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)"""
    install_packages()
    print("\n=== SSADAGU 2ë‹¨ê³„ ê²€ì¦ í¬ë¡¤ëŸ¬ ===")

    crawler = DualVerificationCrawler(use_selenium=True)
    analyzer = DualVerificationAnalyzer()

    TEXT_SIMILARITY_THRESHOLD = 0.6   
    IMAGE_SIMILARITY_THRESHOLD = 0.25 
    MAX_RETRY = 5

    best_match_product = None
    best_match_url = None
    max_combined_score = 0.0

    for attempt in range(MAX_RETRY):
        category_name = random.choice(list(TOP_LEVEL_CATEGORIES.keys()))
        category_id = TOP_LEVEL_CATEGORIES[category_name]
        trending_keywords = search_naver_rank(category_id)

        keyword = random.choice(trending_keywords) if trending_keywords else "ì•…ì„¸ì‚¬ë¦¬"
        print(f"\n[{attempt+1}/{MAX_RETRY}] ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {category_name}, í‚¤ì›Œë“œ: {keyword}")

        # ë„¤ì´ë²„ ì°¸ì¡° ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ
        print("ë„¤ì´ë²„ ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
        reference_features = analyzer.get_reference_features(keyword)
        
        if reference_features is not None:
            print("âœ“ ë„¤ì´ë²„ ì°¸ì¡° ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ")
        else:
            print("âœ— ë„¤ì´ë²„ ì°¸ì¡° ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ â†’ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©")

        # ê²€ìƒ‰
        search_results_urls = (
            crawler.search_products_selenium(keyword)
            if crawler.use_selenium
            else crawler.search_products_requests(keyword)
        )

        if not search_results_urls:
            print("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„")
            continue

        try:
            keyword_embedding = analyzer.get_embedding(keyword)
            temp_best_url = None
            temp_max_score = 0.0

            for i, url in enumerate(search_results_urls[:15]):
                basic_data = crawler.crawl_product_basic(url)
                if not basic_data or basic_data['title'] == "ì œëª© ì—†ìŒ":
                    continue

                # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ì¦
                title_embedding = analyzer.get_embedding(basic_data['title'])
                text_similarity = cosine_similarity(keyword_embedding, title_embedding)[0][0]
                
                print(f"ìƒí’ˆ {i+1}: {basic_data['title'][:30]} | í…ìŠ¤íŠ¸ ìœ ì‚¬ë„: {text_similarity:.4f}")

                # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ 1ë‹¨ê³„ í†µê³¼ ì²´í¬
                if text_similarity >= TEXT_SIMILARITY_THRESHOLD:
                    print("  âœ“ 1ë‹¨ê³„(í…ìŠ¤íŠ¸) í†µê³¼ â†’ 2ë‹¨ê³„(ì´ë¯¸ì§€) ê²€ì¦ ì¤‘...")
                    
                    # 2ë‹¨ê³„: ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì¦
                    image_similarity = crawler.calculate_image_similarity_for_product(
                        analyzer, reference_features, url
                    )
                    
                    print(f"  ì´ë¯¸ì§€ ìœ ì‚¬ë„: {image_similarity:.4f}")
                    
                    if image_similarity >= IMAGE_SIMILARITY_THRESHOLD:
                        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ 60% + ì´ë¯¸ì§€ 40%)
                        combined_score = (text_similarity * 0.6) + (image_similarity * 0.4)
                        print(f"  âœ“ 2ë‹¨ê³„(ì´ë¯¸ì§€) í†µê³¼! ì¢…í•©ì ìˆ˜: {combined_score:.4f}")
                        
                        if combined_score > temp_max_score:
                            temp_max_score = combined_score
                            temp_best_url = url
                    else:
                        print(f"  âœ— 2ë‹¨ê³„(ì´ë¯¸ì§€) ì‹¤íŒ¨ (< {IMAGE_SIMILARITY_THRESHOLD})")
                else:
                    print(f"  âœ— 1ë‹¨ê³„(í…ìŠ¤íŠ¸) ì‹¤íŒ¨ (< {TEXT_SIMILARITY_THRESHOLD})")

            # 2ë‹¨ê³„ ëª¨ë‘ í†µê³¼í•œ ìƒí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
            if temp_best_url and temp_max_score > max_combined_score:
                best_match_url = temp_best_url
                max_combined_score = temp_max_score
                print(f"\nğŸ¯ 2ë‹¨ê³„ ê²€ì¦ í†µê³¼! ì¢…í•©ì ìˆ˜: {max_combined_score:.4f}")
                break
            else:
                print(f"2ë‹¨ê³„ ê²€ì¦ í†µê³¼ ìƒí’ˆ ì—†ìŒ â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„")

        except Exception as e:
            print(f"ê²€ì¦ ê³¼ì • ì˜¤ë¥˜: {e}")
            continue

    # ìµœì¢… ìƒí’ˆ í¬ë¡¤ë§
    if best_match_url:
        print(f"\n=== ìµœì¢… ì„ íƒëœ ìƒí’ˆ ìƒì„¸ í¬ë¡¤ë§ ===")
        best_match_product = crawler.crawl_product_detail(best_match_url, include_images=True)
        if best_match_product:
            # â­ í•µì‹¬ ìˆ˜ì •: float32ë¥¼ ì¼ë°˜ floatë¡œ ë³€í™˜
            best_match_product['text_image_combined_score'] = float(max_combined_score)
            
            print(f"\nğŸ‰ 2ë‹¨ê³„ ê²€ì¦ ì™„ë£Œ!")
            print(f"ì œëª©: {best_match_product['title']}")
            print(f"ê°€ê²©: {best_match_product['price']}ì›")
            print(f"ì¢…í•©ì ìˆ˜: {max_combined_score:.4f}")

            output_filename = f"dual_verification_result_{int(time.time())}.json"
            
            # â­ í•µì‹¬ ìˆ˜ì •: NumpyEncoder ì‚¬ìš©
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump(best_match_product, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
            print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_filename}")
            
            return best_match_product
        else:
            print("ìµœì¢… ìƒí’ˆ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨")
    else:
        print("\nğŸ˜ 2ë‹¨ê³„ ê²€ì¦ì„ í†µê³¼í•œ ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
        print("ê¸°ì¡´ ë°©ì‹(í…ìŠ¤íŠ¸ë§Œ)ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„...")
        fallback_result = main_merged()
        return fallback_result

    return best_match_product

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    main_dual_verification()