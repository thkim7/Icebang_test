import requests
import urllib.parse
from bs4 import BeautifulSoup
import re
import json
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import subprocess
import sys
import csv

from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# SSADAGUCrawler í´ë˜ìŠ¤
class SSADAGUCrawler:
    def __init__(self, use_selenium=True):
        self.base_url = "https://ssadagu.kr"
        self.use_selenium = use_selenium
        if use_selenium:
            self.setup_selenium()
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
    def setup_selenium(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, 10)
            print("Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("requests ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.use_selenium = False
            self.session = requests.Session()

    def search_products_selenium(self, keyword):
        """Seleniumì„ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰ - ê¸°ì¡´ ë°©ì‹ ìœ ì§€"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            self.driver.get(search_url)
            time.sleep(5)
            product_links = []
            link_elements = self.driver.find_elements(By.TAG_NAME, "a")
            for element in link_elements:
                href = element.get_attribute('href')
                if href and 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    product_links.append(href)
            product_links = list(set(product_links))
            print(f"Seleniumìœ¼ë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"Selenium ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_products_requests(self, keyword):
        """requestsë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰ - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë³µì›"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            response = self.session.get(search_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            product_links = []
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                if 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                    product_links.append(full_url)
            print(f"requestsë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"requests ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def crawl_product_basic(self, product_url):
        """ê¸°ë³¸ ìƒí’ˆ ì •ë³´ë§Œ í¬ë¡¤ë§ (ìœ ì‚¬ë„ ë¶„ì„ìš©)"""
        try:
            if self.use_selenium:
                self.driver.get(product_url)
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            title_element = soup.find('h1', {'id': 'kakaotitle'})
            title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
            
            price = 0
            price_selectors = [
                'span.price.gsItemPriceKWR',
                '.pdt_price span.price',
                'span.price',
                '.price'
            ]
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        price = int(price_match.group(1))
                        break

            rating = self.calculate_rating(soup)
            
            return {
                'url': product_url,
                'title': title,
                'price': price,
                'rating': rating
            }
        except Exception as e:
            print(f"ê¸°ë³¸ ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def crawl_product_detail(self, product_url, include_images=True):
        """ìƒì„¸ ìƒí’ˆ ì •ë³´ í¬ë¡¤ë§ (ìµœì¢… ì„ íƒëœ ìƒí’ˆìš©)"""
        try:
            if self.use_selenium:
                self.driver.get(product_url)
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            title_element = soup.find('h1', {'id': 'kakaotitle'})
            title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
            
            price = 0
            price_selectors = [
                'span.price.gsItemPriceKWR',
                '.pdt_price span.price',
                'span.price',
                '.price'
            ]
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        price = int(price_match.group(1))
                        break

            rating = self.calculate_rating(soup)
            options = self.extract_product_options(soup)
            material_info = self.extract_material_info(soup)
            
            product_data = {
                'url': product_url,
                'title': title,
                'price': price,
                'rating': rating,
                'options': options,
                'material_info': material_info,
                'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” ì„ íƒì ìœ¼ë¡œë§Œ ì§„í–‰
            if include_images:
                print("ì´ë¯¸ì§€ OCR ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                product_images = self.extract_product_images(soup)
                translated_images = []
                for img_url in product_images:
                    translated_text = ocr_and_translate_image(img_url)
                    translated_images.append({
                        'original_url': img_url,
                        'translated_text': translated_text
                    })
                product_data['product_images'] = translated_images
            else:
                product_data['product_images'] = []
                
            return product_data
        except Exception as e:
            print(f"ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def calculate_rating(self, soup):
        rating = 0.0
        star_containers = [
            soup.find('a', class_='start'),
            soup.find('div', class_=re.compile(r'star|rating')),
            soup.find('a', href='#reviews_wrap')
        ]
        for container in star_containers:
            if container:
                star_imgs = container.find_all('img')
                for img in star_imgs:
                    src = img.get('src', '')
                    if 'icon_star.svg' in src:
                        rating += 1
                    elif 'icon_star_half.svg' in src:
                        rating += 0.5
                break
        return rating

    def extract_product_options(self, soup):
        options = []
        sku_list = soup.find('ul', {'id': 'skubox'})
        if sku_list:
            option_items = sku_list.find_all('li', class_=re.compile(r'imgWrapper'))
            for item in option_items:
                title_element = item.find('a', title=True)
                if title_element:
                    option_name = title_element.get('title', '').strip()
                    stock = 0
                    item_text = item.get_text()
                    stock_match = re.search(r'ì¬ê³ \s*:\s*(\d+)', item_text)
                    if stock_match:
                        stock = int(stock_match.group(1))
                    img_element = item.find('img', class_='colorSpec_hashPic')
                    image_url = ""
                    if img_element and img_element.get('src'):
                        image_url = img_element['src']
                    if option_name:
                        options.append({
                            'name': option_name,
                            'stock': stock,
                            'image_url': image_url
                        })
        return options

    def extract_product_images(self, soup):
        images = []
        img_elements = soup.find_all('img', {'id': re.compile(r'img_translate_\d+')})
        for img in img_elements:
            src = img.get('src', '')
            if src:
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = self.base_url + src
                elif src.startswith('http'):
                    pass
                else:
                    continue
                images.append(src)
        return images

    def extract_material_info(self, soup):
        material_info = {}
        info_items = soup.find_all('div', class_='pro-info-item')
        for item in info_items:
            title_element = item.find('div', class_='pro-info-title')
            info_element = item.find('div', class_='pro-info-info')
            if title_element and info_element:
                title = title_element.get_text(strip=True)
                info = info_element.get_text(strip=True)
                material_info[title] = info
        return material_info

    def crawl_search_results(self, keyword, max_products=5):
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìƒí’ˆë“¤ í¬ë¡¤ë§ - ê¸°ì¡´ ë°©ì‹ ìœ ì§€"""
        print(f"'{keyword}' ê²€ìƒ‰ ì‹œì‘...")
        if self.use_selenium:
            product_links = self.search_products_selenium(keyword)
        else:
            product_links = self.search_products_requests(keyword)
        
        if not product_links:
            print("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"{len(product_links)}ê°œì˜ ìƒí’ˆ ë§í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        crawled_products = []
        for i, link in enumerate(product_links[:max_products]):
            print(f"\nìƒí’ˆ {i+1}/{min(len(product_links), max_products)} í¬ë¡¤ë§ ì¤‘...")
            product_data = self.crawl_product_detail(link, include_images=True)  # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ì§€ í¬í•¨
            if product_data:
                crawled_products.append(product_data)
                print(f"âœ“ í¬ë¡¤ë§ ì„±ê³µ: {product_data['title'][:50]}...")
            else:
                print("âœ— í¬ë¡¤ë§ ì‹¤íŒ¨")
            time.sleep(random.uniform(2, 4))
        return crawled_products

    def __del__(self):
        if hasattr(self, 'driver'):
            try:
                self.driver.quit()
            except:
                pass

# AI ëª¨ë¸ì„ í™œìš©í•œ ìœ ì‚¬ë„ ë¶„ì„ í•¨ìˆ˜
class SimilarityAnalyzer:
    def __init__(self):
        try:
            # ë” ì•ˆì •ì ì¸ í•œêµ­ì–´ BERT ëª¨ë¸ ì‚¬ìš©
            self.tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
            self.model = AutoModel.from_pretrained('klue/bert-base')
            print("KLUE BERT ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        except Exception as e:
            print(f"KLUE BERT ë¡œë”© ì‹¤íŒ¨, ë‹¤êµ­ì–´ BERTë¡œ ëŒ€ì²´: {e}")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
                self.model = AutoModel.from_pretrained('bert-base-multilingual-cased')
                print("ë‹¤êµ­ì–´ BERT ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e2:
                print(f"ëª¨ë“  BERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e2}")
                raise e2
    
    def get_embedding(self, text):
        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :].numpy()

    def get_similarity(self, text1, text2):
        embedding1 = self.get_embedding(text1)
        embedding2 = self.get_embedding(text2)
        return cosine_similarity(embedding1, embedding2)[0][0]

# ì´ë¯¸ì§€ ë²ˆì—­ ê¸°ëŠ¥ (ê¸°ì¡´ í•¨ìˆ˜)
def ocr_and_translate_image(image_url):
    print(f"ì´ë¯¸ì§€ ë²ˆì—­ ì‹œë„: {image_url}")
    try:
        translated_text = "ë²ˆì—­ëœ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì˜ˆì‹œì…ë‹ˆë‹¤."
        print(f"âœ“ ë²ˆì—­ ì„±ê³µ: '{translated_text[:20]}...'")
        return translated_text
    except Exception as e:
        print(f"âœ— ì´ë¯¸ì§€ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return None

# ì„¤ì¹˜ í•¨ìˆ˜ ë° ë„¤ì´ë²„ ë°ì´í„°ë© í•¨ìˆ˜
def install_packages():
    try:
        print("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        packages = [
            "beautifulsoup4", 
            "requests", 
            "selenium", 
            "torch", 
            "transformers", 
            "numpy", 
            "scikit-learn",
            "protobuf"  # protobuf ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
        ]
        subprocess.check_call([sys.executable, "-m", "pip", "install"] + packages)
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except subprocess.CalledProcessError as e:
        print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("pip install beautifulsoup4 requests selenium torch transformers numpy scikit-learn protobuf")
        sys.exit(1)

TOP_LEVEL_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000", "íŒ¨ì…˜ì¡í™”": "50000001", "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003", "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004", "ì¶œì‚°/ìœ¡ì•„": "50000005",
    "ì‹í’ˆ": "50000006", "ìŠ¤í¬ì¸ /ë ˆì €": "50000007", "ìƒí™œ/ê±´ê°•": "50000008",
    "ì—¬ê°€/ìƒí™œí¸ì˜": "50000009", "ë©´ì„¸ì ": "50000010", "ë„ì„œ": "50005542"
}

def search_naver_rank(category_id):
    url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    headers = {
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://datalab.naver.com/shoppingInsight/sCategory.naver",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }
    keywords = []
    payload = {
        "cid": category_id,
        "timeUnit": "date",
        "startDate": "2025-08-28",
        "endDate": "2025-08-29",
        "age": "",
        "gender": "",
        "device": "",
        "page": 1,
    }
    try:
        response = requests.post(url, headers=headers, data=payload)
        response.raise_for_status()
        data = response.json()
        for item in data.get('ranks', []):
            keywords.append(item.get('keyword'))
    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
        print(f"ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    return keywords

def main_merged():
    install_packages()

    print("\n=== SSADAGU í†µí•© í¬ë¡¤ëŸ¬ ===")
    
    # 1. ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ëœë¤ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    category_name = random.choice(list(TOP_LEVEL_CATEGORIES.keys()))
    category_id = TOP_LEVEL_CATEGORIES[category_name]
    print(f"ğŸŒŸ ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: '{category_name}'")
    trending_keywords = search_naver_rank(category_id)
    if not trending_keywords:
        print("ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 'ì•…ì„¸ì‚¬ë¦¬'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        keyword = "ì•…ì„¸ì‚¬ë¦¬"
    else:
        keyword = trending_keywords[0] # ê°€ì¥ ì¸ê¸° ìˆëŠ” í‚¤ì›Œë“œ 1ê°œë§Œ ì‚¬ìš©
    print(f"ğŸ” ì„ íƒëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: '{keyword}'")
    
    # ê¸°ë³¸ê°’ì„ Seleniumìœ¼ë¡œ ì„¤ì •
    crawler = SSADAGUCrawler(use_selenium=True) 
    
    # 2. ì‹¸ë‹¤êµ¬ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©)
    print(f"\n'{keyword}' í‚¤ì›Œë“œë¡œ ì‹¸ë‹¤êµ¬ì—ì„œ ê²€ìƒ‰ ì‹œì‘...")
    
    # ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
    if crawler.use_selenium:
        search_results_urls = crawler.search_products_selenium(keyword)
    else:
        search_results_urls = crawler.search_products_requests(keyword)
    
    if not search_results_urls:
        print("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ {len(search_results_urls)}ê°œì˜ ìƒí’ˆ URLì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    # 3. AI ëª¨ë¸ì„ ì´ìš©í•œ ìœ ì‚¬ë„ ë¶„ì„ (ì´ë¯¸ì§€ OCR ì œì™¸)
    print("AI ëª¨ë¸ë¡œ ìœ ì‚¬ë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    best_match_product = None
    max_similarity = 0.0  # ê¸°ë³¸ê°’ ì„¤ì •
    best_match_url = None
    
    try:
        analyzer = SimilarityAnalyzer()
        
        # ê¸°ì¤€ í‚¤ì›Œë“œì˜ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°
        keyword_embedding = analyzer.get_embedding(keyword)
        
        # URL ëª©ë¡ì„ ìˆœíšŒí•˜ë©° ê° ìƒí’ˆì˜ ì œëª©ì„ ì–»ê³  ìœ ì‚¬ë„ ë¶„ì„ (ì´ë¯¸ì§€ OCR ì—†ì´)
        for i, url in enumerate(search_results_urls[:5]):  # ìµœëŒ€ 5ê°œë§Œ ë¶„ì„
            print(f"\nìƒí’ˆ {i+1}/{min(len(search_results_urls), 5)} ë¶„ì„ ì¤‘...")
            
            # ê¸°ë³¸ ì •ë³´ë§Œ í¬ë¡¤ë§ (ì´ë¯¸ì§€ ì²˜ë¦¬ ì œì™¸)
            basic_data = crawler.crawl_product_basic(url)
            if not basic_data:
                print("âœ— ìƒí’ˆ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                continue

            title = basic_data['title']
            title_embedding = analyzer.get_embedding(title)
            
            similarity = cosine_similarity(keyword_embedding, title_embedding)[0][0]
            print(f"ìƒí’ˆëª…: '{title[:30]}...' ìœ ì‚¬ë„: {similarity:.4f}")
            
            if similarity > max_similarity:
                max_similarity = similarity
                best_match_url = url
            
            time.sleep(random.uniform(1, 2))  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            
    except Exception as e:
        print(f"ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ì‹œ ì²« ë²ˆì§¸ ìƒí’ˆì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        if search_results_urls:
            best_match_url = search_results_urls[0]
            max_similarity = 0.0  # ê¸°ë³¸ê°’ ì„¤ì •

    # 4. ìµœì¢… ì„ íƒëœ ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ì´ë¯¸ì§€ OCR í¬í•¨)
    if best_match_url:
        print(f"\nâ­ ê°€ì¥ ìœ ì‚¬í•œ ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤...")
        best_match_product = crawler.crawl_product_detail(best_match_url, include_images=True)
        
        if best_match_product:
            print(f"ì œëª©: {best_match_product['title']}")
            print(f"ê°€ê²©: {best_match_product['price']}ì›")
            print(f"ë³„ì : {best_match_product['rating']}/5.0")
            print(f"ì˜µì…˜ ê°œìˆ˜: {len(best_match_product['options'])}ê°œ")
            if max_similarity > 0:
                print(f"ìœ ì‚¬ë„: {max_similarity:.4f}")
            
            print("\nìƒí’ˆ ì´ë¯¸ì§€ (ë²ˆì—­ëœ í…ìŠ¤íŠ¸ í¬í•¨):")
            for j, img_info in enumerate(best_match_product['product_images'], 1):
                print(f"  {j}. URL: {img_info['original_url']}")
                print(f"     ë²ˆì—­ í…ìŠ¤íŠ¸: {img_info['translated_text']}")
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            output_filename = f"ssadagu_best_match_{int(time.time())}.json"
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump([best_match_product], f, ensure_ascii=False, indent=2)
            print(f"\nê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("ìµœì¢… ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\në¶„ì„í•  ìˆ˜ ìˆëŠ” ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    main_merged()