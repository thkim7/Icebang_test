import requests
import urllib.parse
from bs4 import BeautifulSoup
import re
import json
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import subprocess
import sys
import csv

from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ì¸ì½”ë” í´ë˜ìŠ¤
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

# SSADAGUCrawler í´ë˜ìŠ¤ (KoNLPy ì˜¤ë¥˜ ìˆ˜ì •)
class SSADAGUCrawler:
    def __init__(self, use_selenium=True):
        self.base_url = "https://ssadagu.kr"
        self.use_selenium = use_selenium
        self.konlpy_available = False
        
        # KoNLPy ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        try:
            from konlpy.tag import Okt
            test_okt = Okt()
            test_result = test_okt.morphs("í…ŒìŠ¤íŠ¸")
            if test_result:
                self.konlpy_available = True
                print("KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© ê°€ëŠ¥")
        except Exception as e:
            print(f"KoNLPy ì‚¬ìš© ë¶ˆê°€ (ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´): {e}")
        
        if use_selenium:
            self.setup_selenium()
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
    def setup_selenium(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, 10)
            print("Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("requests ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.use_selenium = False
            self.session = requests.Session()

    def search_products_selenium(self, keyword):
        """Seleniumì„ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            self.driver.get(search_url)
            time.sleep(5)
            product_links = []
            link_elements = self.driver.find_elements(By.TAG_NAME, "a")
            for element in link_elements:
                href = element.get_attribute('href')
                if href and 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    product_links.append(href)
            product_links = list(set(product_links))
            print(f"Seleniumìœ¼ë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"Selenium ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_products_requests(self, keyword):
        """requestsë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            response = self.session.get(search_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            product_links = []
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                if 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                    product_links.append(full_url)
            print(f"requestsë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"requests ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def crawl_product_basic(self, product_url):
        """ê¸°ë³¸ ìƒí’ˆ ì •ë³´ë§Œ í¬ë¡¤ë§"""
        try:
            if self.use_selenium:
                self.driver.get(product_url)
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            title_element = soup.find('h1', {'id': 'kakaotitle'})
            title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
            
            price = 0
            price_selectors = [
                'span.price.gsItemPriceKWR',
                '.pdt_price span.price',
                'span.price',
                '.price'
            ]
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        price = int(price_match.group(1))
                        break

            rating = self.calculate_rating(soup)
            
            return {
                'url': product_url,
                'title': title,
                'price': price,
                'rating': rating
            }
        except Exception as e:
            print(f"ê¸°ë³¸ ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def crawl_product_detail(self, product_url, include_images=False):
        """ìƒì„¸ ìƒí’ˆ ì •ë³´ í¬ë¡¤ë§"""
        try:
            if self.use_selenium:
                self.driver.get(product_url)
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                response = requests.get(product_url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            title_element = soup.find('h1', {'id': 'kakaotitle'})
            title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
            
            price = 0
            price_selectors = [
                'span.price.gsItemPriceKWR',
                '.pdt_price span.price',
                'span.price',
                '.price'
            ]
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        price = int(price_match.group(1))
                        break

            rating = self.calculate_rating(soup)
            options = self.extract_product_options(soup)
            material_info = self.extract_material_info(soup)
            
            product_data = {
                'url': product_url,
                'title': title,
                'price': price,
                'rating': rating,
                'options': options,
                'material_info': material_info,
                'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            if include_images:
                print("ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì¤‘...")
                product_images = self.extract_product_images(soup)
                product_data['product_images'] = [{'original_url': img_url} for img_url in product_images]
            else:
                product_data['product_images'] = []
                
            return product_data
        except Exception as e:
            print(f"ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def calculate_rating(self, soup):
        rating = 0.0
        star_containers = [
            soup.find('a', class_='start'),
            soup.find('div', class_=re.compile(r'star|rating')),
            soup.find('a', href='#reviews_wrap')
        ]
        for container in star_containers:
            if container:
                star_imgs = container.find_all('img')
                for img in star_imgs:
                    src = img.get('src', '')
                    if 'icon_star.svg' in src:
                        rating += 1
                    elif 'icon_star_half.svg' in src:
                        rating += 0.5
                break
        return rating

    def extract_product_options(self, soup):
        options = []
        sku_list = soup.find('ul', {'id': 'skubox'})
        if sku_list:
            option_items = sku_list.find_all('li', class_=re.compile(r'imgWrapper'))
            for item in option_items:
                title_element = item.find('a', title=True)
                if title_element:
                    option_name = title_element.get('title', '').strip()
                    stock = 0
                    item_text = item.get_text()
                    stock_match = re.search(r'ì¬ê³ \s*:\s*(\d+)', item_text)
                    if stock_match:
                        stock = int(stock_match.group(1))
                    img_element = item.find('img', class_='colorSpec_hashPic')
                    image_url = ""
                    if img_element and img_element.get('src'):
                        image_url = img_element['src']
                    if option_name:
                        options.append({
                            'name': option_name,
                            'stock': stock,
                            'image_url': image_url
                        })
        return options

    def extract_product_images(self, soup):
        images = []
        img_elements = soup.find_all('img', {'id': re.compile(r'img_translate_\d+')})
        for img in img_elements:
            src = img.get('src', '')
            if src:
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = self.base_url + src
                elif src.startswith('http'):
                    pass
                else:
                    continue
                images.append(src)
        return images

    def extract_material_info(self, soup):
        material_info = {}
        info_items = soup.find_all('div', class_='pro-info-item')
        for item in info_items:
            title_element = item.find('div', class_='pro-info-title')
            info_element = item.find('div', class_='pro-info-info')
            if title_element and info_element:
                title = title_element.get_text(strip=True)
                info = info_element.get_text(strip=True)
                material_info[title] = info
        return material_info

    def contains_keyword(self, title, keyword):
        """ì•ˆì „í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (KoNLPy ì˜¤ë¥˜ ë°©ì§€)"""
        title_lower = title.lower().strip()
        keyword_lower = keyword.lower().strip()
        
        # 1. ì™„ì „ í¬í•¨ ê²€ì‚¬
        if keyword_lower in title_lower:
            return True
        
        # 2. í˜•íƒœì†Œ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
        try:
            if self.konlpy_available:
                from konlpy.tag import Okt
                okt = Okt()
                
                keyword_morphs = okt.nouns(keyword_lower)
                if not keyword_morphs:  # ëª…ì‚¬ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ í˜•íƒœì†Œ
                    keyword_morphs = okt.morphs(keyword_lower)
                
                title_morphs = okt.nouns(title_lower)
                if not title_morphs:
                    title_morphs = okt.morphs(title_lower)
                
                # í˜•íƒœì†Œ ë§¤ì¹­
                matched = 0
                for kw in keyword_morphs:
                    if len(kw) >= 2:
                        for tw in title_morphs:
                            if kw == tw or kw in tw or tw in kw:
                                matched += 1
                                break
                
                match_ratio = matched / len(keyword_morphs) if keyword_morphs else 0
                if match_ratio >= 0.4:
                    print(f"    í˜•íƒœì†Œ ë§¤ì¹­ ì„±ê³µ: {matched}/{len(keyword_morphs)} = {match_ratio:.3f}")
                    return True
                    
        except Exception as e:
            print(f"    í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´: {e}")
        
        # 3. ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (KoNLPy ì‹¤íŒ¨ì‹œ)
        return self._simple_keyword_match(title_lower, keyword_lower)
    
    def _simple_keyword_match(self, title, keyword):
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­"""
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        title_words = title.split()
        keyword_words = keyword.split()
        
        matched = 0
        for kw in keyword_words:
            if len(kw) >= 2:
                for tw in title_words:
                    if kw in tw or tw in kw:
                        matched += 1
                        break
        
        match_ratio = matched / len(keyword_words) if keyword_words else 0
        return match_ratio >= 0.3

    def __del__(self):
        if hasattr(self, 'driver'):
            try:
                self.driver.quit()
            except:
                pass

# í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„ê¸°
class SimilarityAnalyzer:
    def __init__(self):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
            self.model = AutoModel.from_pretrained('klue/bert-base')
            print("KLUE BERT ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        except Exception as e:
            print(f"KLUE BERT ë¡œë”© ì‹¤íŒ¨, ë‹¤êµ­ì–´ BERTë¡œ ëŒ€ì²´: {e}")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
                self.model = AutoModel.from_pretrained('bert-base-multilingual-cased')
                print("ë‹¤êµ­ì–´ BERT ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e2:
                print(f"ëª¨ë“  BERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e2}")
                raise e2
    
    def get_embedding(self, text):
        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :].numpy()

    def get_similarity(self, text1, text2):
        embedding1 = self.get_embedding(text1)
        embedding2 = self.get_embedding(text2)
        return cosine_similarity(embedding1, embedding2)[0][0]

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•¨ìˆ˜
def install_packages():
    try:
        print("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        packages = [
            "beautifulsoup4", 
            "requests", 
            "selenium", 
            "torch", 
            "transformers", 
            "numpy", 
            "scikit-learn",
            "protobuf"
        ]
        subprocess.check_call([sys.executable, "-m", "pip", "install"] + packages)
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # KoNLPyëŠ” ì„ íƒì  ì„¤ì¹˜
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "konlpy"])
            print("KoNLPy ì„¤ì¹˜ ì„±ê³µ")
        except:
            print("KoNLPy ì„¤ì¹˜ ì‹¤íŒ¨ (ì„ íƒì‚¬í•­) - ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´")
            
    except subprocess.CalledProcessError as e:
        print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("pip install beautifulsoup4 requests selenium torch transformers numpy scikit-learn protobuf")
        sys.exit(1)

# ë„¤ì´ë²„ ë°ì´í„°ë©
TOP_LEVEL_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000", "íŒ¨ì…˜ì¡í™”": "50000001", "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003", "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004", "ì¶œì‚°/ìœ¡ì•„": "50000005",
    "ì‹í’ˆ": "50000006", "ìŠ¤í¬ì¸ /ë ˆì €": "50000007", "ìƒí™œ/ê±´ê°•": "50000008",
    "ì—¬ê°€/ìƒí™œí¸ì˜": "50000009", "ë©´ì„¸ì ": "50000010", "ë„ì„œ": "50005542"
}

def search_naver_rank(category_id):
    url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    headers = {
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://datalab.naver.com/shoppingInsight/sCategory.naver",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }
    keywords = []
    payload = {
        "cid": category_id,
        "timeUnit": "date",
        "startDate": "2025-08-28",
        "endDate": "2025-08-29",
        "age": "",
        "gender": "",
        "device": "",
        "page": 1,
    }
    try:
        response = requests.post(url, headers=headers, data=payload)
        response.raise_for_status()
        data = response.json()
        for item in data.get('ranks', []):
            keywords.append(item.get('keyword'))
    except (requests.exceptions.RequestException, json.JSONDecodeError) as e:
        print(f"ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    return keywords

# ë©”ì¸ í•¨ìˆ˜ (ì›ë˜ëŒ€ë¡œ ë‹¨ìˆœí•˜ê²Œ)
def main_simplified():
    """ì›ë˜ ì½”ë“œì™€ ë™ì¼í•œ ë‹¨ìˆœí•œ í¬ë¡¤ëŸ¬ - KoNLPy ì˜¤ë¥˜ë§Œ ìˆ˜ì •"""
    install_packages()
    print("\n=== SSADAGU í¬ë¡¤ëŸ¬ (KoNLPy ì˜¤ë¥˜ ìˆ˜ì •) ===")

    crawler = SSADAGUCrawler(use_selenium=True)
    analyzer = SimilarityAnalyzer()

    TEXT_SIMILARITY_THRESHOLD = 0.6
    MAX_RETRY = 5

    best_match_product = None
    best_match_url = None

    for attempt in range(MAX_RETRY):
        category_name = random.choice(list(TOP_LEVEL_CATEGORIES.keys()))
        category_id = TOP_LEVEL_CATEGORIES[category_name]
        trending_keywords = search_naver_rank(category_id)

        keyword = random.choice(trending_keywords) if trending_keywords else "ì•…ì„¸ì‚¬ë¦¬"
        print(f"\n[{attempt+1}/{MAX_RETRY}] ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {category_name}, í‚¤ì›Œë“œ: {keyword}")

        # ê²€ìƒ‰
        search_results_urls = (
            crawler.search_products_selenium(keyword)
            if crawler.use_selenium
            else crawler.search_products_requests(keyword)
        )

        if not search_results_urls:
            print("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„")
            continue

        print(f"ì´ {len(search_results_urls)}ê°œ ìƒí’ˆ ê²€ìƒ‰ë¨, ìµœëŒ€ 20ê°œê¹Œì§€ ë¶„ì„")

        try:
            # 1ë‹¨ê³„: ì „ì²´ ìƒí’ˆì—ì„œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            all_products = []
            keyword_included_products = []
            
            for i, url in enumerate(search_results_urls[:20]):
                basic_data = crawler.crawl_product_basic(url)
                if not basic_data or basic_data['title'] == "ì œëª© ì—†ìŒ":
                    continue
                
                print(f"ìƒí’ˆ {i+1}: {basic_data['title'][:50]}")
                all_products.append(basic_data)
                
                # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ìˆ˜ì •ëœ ë§¤ì¹­ ì‚¬ìš©)
                if crawler.contains_keyword(basic_data['title'], keyword):
                    keyword_included_products.append(basic_data)
                    print(f"  ğŸ” í‚¤ì›Œë“œ '{keyword}' ë§¤ì¹­ë¨!")

            print(f"\nì „ì²´ ìœ íš¨ ìƒí’ˆ: {len(all_products)}ê°œ")
            print(f"í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ: {len(keyword_included_products)}ê°œ")

            # 2ë‹¨ê³„: ì„ íƒ ë¡œì§
            selected_product = None
            selection_reason = ""

            if len(keyword_included_products) == 1:
                selected_product = keyword_included_products[0]
                selection_reason = "í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ 1ê°œ â†’ ë°”ë¡œ ì„ íƒ"
                print(f"âœ… {selection_reason}")
                
            elif len(keyword_included_products) > 1:
                print("ğŸ”„ í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ ì—¬ëŸ¬ê°œ â†’ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ")
                keyword_embedding = analyzer.get_embedding(keyword)
                best_similarity = 0.0
                
                for product in keyword_included_products:
                    title_embedding = analyzer.get_embedding(product['title'])
                    similarity = cosine_similarity(keyword_embedding, title_embedding)[0][0]
                    print(f"  {product['title'][:40]} | ìœ ì‚¬ë„: {similarity:.4f}")
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        selected_product = product
                        
                selection_reason = f"í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ ì¤‘ ìµœê³  ìœ ì‚¬ë„({best_similarity:.4f})"
                print(f"âœ… {selection_reason}")
                
            elif len(keyword_included_products) == 0:
                print("ğŸ”„ í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ ì—†ìŒ â†’ ì „ì²´ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ì¦")
                keyword_embedding = analyzer.get_embedding(keyword)
                best_similarity = 0.0
                
                for product in all_products:
                    title_embedding = analyzer.get_embedding(product['title'])
                    similarity = cosine_similarity(keyword_embedding, title_embedding)[0][0]
                    print(f"  {product['title'][:40]} | ìœ ì‚¬ë„: {similarity:.4f}")
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        selected_product = product
                
                if best_similarity >= TEXT_SIMILARITY_THRESHOLD:
                    selection_reason = f"ì „ì²´ ê²€ì¦ ì¤‘ ìµœê³  ìœ ì‚¬ë„({best_similarity:.4f}) ê¸°ì¤€ í†µê³¼"
                    print(f"âœ… {selection_reason}")
                else:
                    print(f"âŒ ìµœê³  ìœ ì‚¬ë„({best_similarity:.4f}) < ê¸°ì¤€({TEXT_SIMILARITY_THRESHOLD}) â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ")
                    continue

            # 3ë‹¨ê³„: ì„ íƒëœ ìƒí’ˆì´ ìˆìœ¼ë©´ ìƒì„¸ í¬ë¡¤ë§ í›„ ì¢…ë£Œ
            if selected_product:
                print(f"\nğŸ¯ ìµœì¢… ì„ íƒ: {selected_product['title']}")
                print(f"ì„ íƒ ì´ìœ : {selection_reason}")
                
                # ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
                best_match_product = crawler.crawl_product_detail(selected_product['url'], include_images=True)
                if best_match_product:
                    best_match_product['selection_reason'] = selection_reason
                    best_match_url = selected_product['url']
                    break
                else:
                    print("ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨ â†’ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„")
                    continue

        except Exception as e:
            print(f"ë¶„ì„ ê³¼ì • ì˜¤ë¥˜: {e}")
            continue

    # ìµœì¢… ê²°ê³¼ ì²˜ë¦¬
    if best_match_product:
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ì œëª©: {best_match_product['title']}")
        print(f"ê°€ê²©: {best_match_product['price']}ì›")
        print(f"ë³„ì : {best_match_product['rating']}")
        print(f"ì„ íƒ ì´ìœ : {best_match_product['selection_reason']}")

        output_filename = f"fixed_crawler_result_{int(time.time())}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(best_match_product, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_filename}")
        
        return best_match_product
    else:
        print("\nğŸ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    main_simplified()