import requests
import urllib.parse
from bs4 import BeautifulSoup
import re
import json
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import subprocess
import sys
import csv

# --- ì´ë¯¸ì§€ ë²ˆì—­ ê¸°ëŠ¥ì— ëŒ€í•œ ì£¼ì„ ì¶”ê°€ ---
def ocr_and_translate_image(image_url):
    """
    [ì•ˆë‚´] ì´ í•¨ìˆ˜ëŠ” ê°€ìƒì˜ OCR ë° ë²ˆì—­ ê¸°ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” Google Cloud Vision API, Azure Cognitive Services ë“±
    ì™¸ë¶€ OCR/ë²ˆì—­ APIë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì´ í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    í˜„ì¬ëŠ” ì‹¤ì œ ë²ˆì—­ì´ ì•„ë‹Œ ê°€ìƒì˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"ì´ë¯¸ì§€ ë²ˆì—­ ì‹œë„: {image_url}")
    try:
        translated_text = "ë²ˆì—­ëœ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì˜ˆì‹œì…ë‹ˆë‹¤."
        print(f"âœ“ ë²ˆì—­ ì„±ê³µ: '{translated_text[:20]}...'")
        return translated_text
    except Exception as e:
        print(f"âœ— ì´ë¯¸ì§€ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return None

# --- SSADAGUCrawler í´ë˜ìŠ¤ (crawler.pyì—ì„œ ê°€ì ¸ì˜´) ---
class SSADAGUCrawler:
    def __init__(self, use_selenium=True):
        self.base_url = "https://ssadagu.kr"
        self.use_selenium = use_selenium
        if use_selenium:
            self.setup_selenium()
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })

    def setup_selenium(self):
        """Selenium WebDriver ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, 10)
            print("Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("requests ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.use_selenium = False
            self.session = requests.Session()

    def search_products_selenium(self, keyword):
        """Seleniumì„ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            self.driver.get(search_url)
            time.sleep(5)
            product_links = []
            link_elements = self.driver.find_elements(By.TAG_NAME, "a")
            for element in link_elements:
                href = element.get_attribute('href')
                if href and 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    product_links.append(href)
            product_links = list(set(product_links))
            print(f"Seleniumìœ¼ë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"Selenium ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_products_requests(self, keyword):
        """requestsë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ ê²€ìƒ‰"""
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"{self.base_url}/shop/search.php?ss_tx={encoded_keyword}"
        try:
            response = self.session.get(search_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            product_links = []
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                if 'view.php' in href and ('platform=1688' in href or 'num_iid' in href):
                    full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                    product_links.append(full_url)
            print(f"requestsë¡œ ë°œê²¬í•œ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            return product_links
        except Exception as e:
            print(f"requests ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def calculate_rating(self, soup):
        """ë³„ì  ê³„ì‚° (ë³„=1, ë°˜ë³„=0.5, ë¹ˆë³„=0)"""
        rating = 0.0
        star_containers = [
            soup.find('a', class_='start'),
            soup.find('div', class_=re.compile(r'star|rating')),
            soup.find('a', href='#reviews_wrap')
        ]
        for container in star_containers:
            if container:
                star_imgs = container.find_all('img')
                for img in star_imgs:
                    src = img.get('src', '')
                    if 'icon_star.svg' in src:
                        rating += 1
                    elif 'icon_star_half.svg' in src:
                        rating += 0.5
                break
        return rating

    def extract_product_options(self, soup):
        """ìƒí’ˆ ì˜µì…˜ë“¤ ì¶”ì¶œ"""
        options = []
        sku_list = soup.find('ul', {'id': 'skubox'})
        if sku_list:
            option_items = sku_list.find_all('li', class_=re.compile(r'imgWrapper'))
            for item in option_items:
                title_element = item.find('a', title=True)
                if title_element:
                    option_name = title_element.get('title', '').strip()
                    stock = 0
                    item_text = item.get_text()
                    stock_match = re.search(r'ì¬ê³ \s*:\s*(\d+)', item_text)
                    if stock_match:
                        stock = int(stock_match.group(1))
                    img_element = item.find('img', class_='colorSpec_hashPic')
                    image_url = ""
                    if img_element and img_element.get('src'):
                        image_url = img_element['src']
                    if option_name:
                        options.append({
                            'name': option_name,
                            'stock': stock,
                            'image_url': image_url
                        })
        return options

    def extract_product_images(self, soup):
        """ìƒí’ˆ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ"""
        images = []
        img_elements = soup.find_all('img', {'id': re.compile(r'img_translate_\d+')})
        for img in img_elements:
            src = img.get('src', '')
            if src:
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = self.base_url + src
                elif src.startswith('http'):
                    pass
                else:
                    continue
                images.append(src)
        return images

    def extract_material_info(self, soup):
        """ì¬ë£Œ ë° ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        material_info = {}
        info_items = soup.find_all('div', class_='pro-info-item')
        for item in info_items:
            title_element = item.find('div', class_='pro-info-title')
            info_element = item.find('div', class_='pro-info-info')
            if title_element and info_element:
                title = title_element.get_text(strip=True)
                info = info_element.get_text(strip=True)
                material_info[title] = info
        return material_info

    def crawl_product_detail(self, product_url):
        """ê°œë³„ ìƒí’ˆ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        try:
            if self.use_selenium:
                return self.crawl_with_selenium(product_url)
            else:
                return self.crawl_with_requests(product_url)
        except Exception as e:
            print(f"ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_url}): {e}")
            return None

    def crawl_with_selenium(self, product_url):
        """Seleniumìœ¼ë¡œ ìƒí’ˆ ì •ë³´ í¬ë¡¤ë§"""
        self.driver.get(product_url)
        time.sleep(3)
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        return self.extract_product_data(soup, product_url)

    def crawl_with_requests(self, product_url):
        """requestsë¡œ ìƒí’ˆ ì •ë³´ í¬ë¡¤ë§"""
        response = self.session.get(product_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        return self.extract_product_data(soup, product_url)

    def extract_product_data(self, soup, product_url):
        """soup ê°ì²´ì—ì„œ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ"""
        title_element = soup.find('h1', {'id': 'kakaotitle'})
        title = title_element.get_text(strip=True) if title_element else "ì œëª© ì—†ìŒ"
        if title == "ì œëª© ì—†ìŒ" or not title:
            alt_titles = [
                soup.find('h1'),
                soup.find('title'),
                soup.find('div', class_=re.compile(r'title|name'))
            ]
            for alt_title in alt_titles:
                if alt_title:
                    title = alt_title.get_text(strip=True)
                    if title and title != "ì œëª© ì—†ìŒ":
                        break
        price = 0
        price_selectors = [
            'span.price.gsItemPriceKWR',
            '.pdt_price span.price',
            'span.price',
            '.price'
        ]
        for selector in price_selectors:
            price_element = soup.select_one(selector)
            if price_element:
                price_text = price_element.get_text(strip=True).replace(',', '').replace('ì›', '')
                price_match = re.search(r'(\d+)', price_text)
                if price_match:
                    price = int(price_match.group(1))
                    break
        rating = self.calculate_rating(soup)
        options = self.extract_product_options(soup)
        material_info = self.extract_material_info(soup)
        product_images = self.extract_product_images(soup)

        translated_images = []
        for img_url in product_images:
            translated_text = ocr_and_translate_image(img_url)
            translated_images.append({
                'original_url': img_url,
                'translated_text': translated_text
            })

        product_data = {
            'url': product_url,
            'title': title,
            'price': price,
            'rating': rating,
            'options': options,
            'material_info': material_info,
            'product_images': translated_images,
            'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        return product_data

    def crawl_search_results(self, keyword, max_products=5):
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìƒí’ˆë“¤ í¬ë¡¤ë§"""
        print(f"'{keyword}' ê²€ìƒ‰ ì‹œì‘...")
        if self.use_selenium:
            product_links = self.search_products_selenium(keyword)
        else:
            product_links = self.search_products_requests(keyword)
        
        if not product_links:
            print("ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"{len(product_links)}ê°œì˜ ìƒí’ˆ ë§í¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        crawled_products = []
        for i, link in enumerate(product_links[:max_products]):
            print(f"\nìƒí’ˆ {i+1}/{min(len(product_links), max_products)} í¬ë¡¤ë§ ì¤‘...")
            product_data = self.crawl_product_detail(link)
            if product_data:
                crawled_products.append(product_data)
                print(f"âœ“ í¬ë¡¤ë§ ì„±ê³µ: {product_data['title'][:50]}...")
            else:
                print("âœ— í¬ë¡¤ë§ ì‹¤íŒ¨")
            time.sleep(random.uniform(2, 4))
        return crawled_products

    def __del__(self):
        """ì†Œë©¸ì - Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            try:
                self.driver.quit()
            except:
                pass

def install_packages():
    """í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."""
    try:
        print("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (beautifulsoup4, requests, selenium) ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "beautifulsoup4", "requests", "selenium"])
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except subprocess.CalledProcessError as e:
        print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ 'pip install beautifulsoup4 requests selenium' ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

TOP_LEVEL_CATEGORIES = {
    "íŒ¨ì…˜ì˜ë¥˜": "50000000", "íŒ¨ì…˜ì¡í™”": "50000001", "í™”ì¥í’ˆ/ë¯¸ìš©": "50000002",
    "ë””ì§€í„¸/ê°€ì „": "50000003", "ê°€êµ¬/ì¸í…Œë¦¬ì–´": "50000004", "ì¶œì‚°/ìœ¡ì•„": "50000005",
    "ì‹í’ˆ": "50000006", "ìŠ¤í¬ì¸ /ë ˆì €": "50000007", "ìƒí™œ/ê±´ê°•": "50000008",
    "ì—¬ê°€/ìƒí™œí¸ì˜": "50000009", "ë©´ì„¸ì ": "50000010", "ë„ì„œ": "50005542"
}

def search_naver_rank(category_id):
    """ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ì¸ê¸° ê²€ìƒ‰ì–´ ìˆœìœ„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    headers = {
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://datalab.naver.com/shoppingInsight/sCategory.naver",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }
    keywords = []
    payload = {
        "cid": category_id,
        "timeUnit": "date",
        "startDate": "2025-08-28",
        "endDate": "2025-08-29",
        "age": "",
        "gender": "",
        "device": "",
        "page": 1,
    }
    response = requests.post(url, headers=headers, data=payload)
    if response.status_code == 200:
        try:
            data = response.json()
            for item in data.get('ranks', []):
                keywords.append(item.get('keyword'))
        except json.JSONDecodeError:
            print("JSON ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
    return keywords

# --- ìˆ˜ì •ëœ ë©”ì¸ ë¡œì§ ---
def main_merged():
    install_packages()

    print("\n=== SSADAGU í†µí•© í¬ë¡¤ëŸ¬ ===")
    
    # ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ëœë¤ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    category_name = random.choice(list(TOP_LEVEL_CATEGORIES.keys()))
    category_id = TOP_LEVEL_CATEGORIES[category_name]
    print(f"ğŸŒŸ ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: '{category_name}'")
    trending_keywords = search_naver_rank(category_id)
    if not trending_keywords:
        print("ë„¤ì´ë²„ ë°ì´í„°ë©ì—ì„œ ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 'ì•…ì„¸ì‚¬ë¦¬'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        keyword = "ì•…ì„¸ì‚¬ë¦¬"
    else:
        keyword = random.choice(trending_keywords)
    print(f"ğŸ” ì„ íƒëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: '{keyword}'")
    
    crawler = SSADAGUCrawler(use_selenium=True)
    products = crawler.crawl_search_results(keyword, max_products=1)

    print(f"\n=== í¬ë¡¤ë§ ê²°ê³¼: {len(products)}ê°œ ìƒí’ˆ ===")
    for i, product in enumerate(products, 1):
        if product:
            print(f"\n--- ìƒí’ˆ {i} ---")
            print(f"ì œëª©: {product['title']}")
            print(f"ê°€ê²©: {product['price']}ì›")
            print(f"ë³„ì : {product['rating']}/5.0")
            print(f"ì˜µì…˜ ê°œìˆ˜: {len(product['options'])}ê°œ")
            
            print("ìƒí’ˆ ì´ë¯¸ì§€ (ë²ˆì—­ëœ í…ìŠ¤íŠ¸ í¬í•¨):")
            for j, img_info in enumerate(product['product_images'], 1):
                print(f"  {j}. URL: {img_info['original_url']}")
                print(f"     ë²ˆì—­ í…ìŠ¤íŠ¸: {img_info['translated_text']}")
    
    if products:
        filename_keyword = keyword
        output_filename = f"ssadagu_products_{filename_keyword}_{int(time.time())}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        print(f"\nê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        total_options = sum(len(p['options']) for p in products)
        total_images = sum(len(p['product_images']) for p in products)
        avg_rating = sum(p['rating'] for p in products) / len(products) if products else 0
        print(f"\n=== í¬ë¡¤ë§ í†µê³„ ===")
        print(f"ì´ ìƒí’ˆ ìˆ˜: {len(products)}ê°œ")
        print(f"ì´ ì˜µì…˜ ìˆ˜: {total_options}ê°œ")
        print(f"ì´ ì´ë¯¸ì§€ ìˆ˜: {total_images}ê°œ")
        print(f"í‰ê·  ë³„ì : {avg_rating:.2f}/5.0")
    else:
        print("\ní¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main_merged()